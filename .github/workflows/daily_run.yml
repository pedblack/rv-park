name: Daily Land Search & Map

on:
  schedule:
    - cron: '0 8 * * *'
  workflow_dispatch:
    inputs:
      mode:
        description: 'Run mode'
        required: true
        default: 'prod'
        type: choice
        options: [prod, dev]
      skip_crawl:
        description: 'Skip crawling and only run analysis/visualization'
        required: false
        type: boolean
        default: false
      force_recrawl:
        description: 'Force recrawl stale properties'
        required: false
        type: boolean
        default: false
      url:
        description: 'Optional single location URL to crawl (overrides queue)'
        required: false
        type: string
        default: ''

jobs:
  scrape_and_visualize:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetches all history for all branches and tags

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Dependencies
        run: |
          pip install -r requirements.txt
          pip install scikit-learn 
          playwright install chromium --with-deps

      - name: Run Unified Scraper
        if: github.event.inputs.skip_crawl != 'true'
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          P4N_USERNAME: ${{ secrets.P4N_USERNAME }}
          P4N_PASSWORD: ${{ secrets.P4N_PASSWORD }}
        run: |
          ARGS=()
          if [[ "${{ github.event.inputs.mode }}" == "dev" ]]; then ARGS+=("--dev"); fi
          if [[ "${{ github.event.inputs.force_recrawl }}" == "true" ]]; then ARGS+=("--force"); fi
          if [[ -n "${{ github.event.inputs.url }}" ]]; then ARGS+=("--url" "${{ github.event.inputs.url }}"); fi
          python backbone_crawler.py "${ARGS[@]}"

      - name: Set Intelligence Environment
        # BUG FIX: Persists the correct file names for both following steps
        run: |
          if [ -f "backbone_locations_dev.csv" ]; then
            echo "INPUT_CSV=backbone_locations_dev.csv" >> $GITHUB_ENV
            echo "CSV_FILE=backbone_locations_dev.csv" >> $GITHUB_ENV
          else
            echo "INPUT_CSV=backbone_locations.csv" >> $GITHUB_ENV
            echo "CSV_FILE=backbone_locations.csv" >> $GITHUB_ENV
          fi

      - name: Run Demand Analyzer
        run: python demand_analyzer.py

      - name: Generate Visualization
        run: python visualize_land.py

      - name: Upload Pipeline Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-debug-package
          path: |
            pipeline_execution.log
            strategic_analysis.json
            *.csv
            index.html

      - name: Commit and Push (Prod Only)
        # Run on schedule OR if mode is explicitly set to prod
        if: github.event_name == 'schedule' || github.event.inputs.mode == 'prod'
        run: |
          git config --global user.name "P4N Bot"
          git config --global user.email "bot@github.com"
          
          # Add files only if they exist to prevent "pathspec did not match" errors
          [ -f index.html ] && git add index.html
          [ -f backbone_locations.csv ] && git add backbone_locations.csv
          [ -f queue_state.json ] && git add queue_state.json
          [ -f strategic_analysis.json ] && git add strategic_analysis.json
          
          # Check for actual data changes
          if ! git diff --staged --quiet; then
            git commit -m "Auto-update strategic intelligence [skip ci]"
            # Synchronization fix for concurrent runs
            git pull --rebase origin main
            git push origin main
          else
            echo "No changes detected. Skipping push."
          fi
