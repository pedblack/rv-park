name: Daily Land Search & Map

on:
  schedule:
    - cron: '0 8 * * *'
  workflow_dispatch:
    inputs:
      mode:
        description: 'Run mode'
        required: true
        default: 'prod'
        type: choice
        options: [prod, dev]
      skip_crawl:
        description: 'Skip crawling and only run analysis/visualization'
        required: false
        type: boolean
        default: false
      force_recrawl:
        description: 'Force recrawl stale properties'
        required: false
        type: boolean
        default: false

jobs:
  scrape_and_visualize:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Dependencies
        run: |
          pip install -r requirements.txt
          # Ensure scikit-learn is installed for the analyzer
          pip install scikit-learn 
          playwright install chromium --with-deps

      - name: Run Unified Scraper
        if: github.event.inputs.skip_crawl != 'true'
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          P4N_USERNAME: ${{ secrets.P4N_USERNAME }}
          P4N_PASSWORD: ${{ secrets.P4N_PASSWORD }}
        run: |
          ARGS=""
          if [[ "${{ github.event.inputs.mode }}" == "dev" ]]; then ARGS="$ARGS --dev"; fi
          if [[ "${{ github.event.inputs.force_recrawl }}" == "true" ]]; then ARGS="$ARGS --force"; fi
          python backbone_crawler.py $ARGS

      - name: Run Demand Analyzer
        # The analyzer transforms raw data into strategic intelligence
        # FIRE Metric: Targets 90+ Opportunity Score clusters
        run: |
          if [ -f "backbone_locations_dev.csv" ]; then export INPUT_CSV="backbone_locations_dev.csv"; fi
          python demand_analyzer.py

      - name: Generate Visualization
        run: |
          if [ -f "backbone_locations_dev.csv" ]; then export CSV_FILE="backbone_locations_dev.csv"; fi
          python visualize_land.py

      - name: Upload Pipeline Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-debug-package
          path: |
            pipeline_execution.log
            strategic_analysis.json
            *.png

      - name: Commit and Push (Prod Only)
        if: github.event.inputs.mode != 'dev'
        run: |
          git config --global user.name "P4N Bot"
          git config --global user.email "bot@github.com"
          # Track the strategic JSON to monitor check-in frequency over time
          git add backbone_locations.csv index.html queue_state.json pipeline_execution.log strategic_analysis.json
          git diff --staged --quiet || (git commit -m "Auto-update strategic intelligence [skip ci]" && git push)
