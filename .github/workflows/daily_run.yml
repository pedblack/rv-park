name: Daily Land Search & Map

on:
  schedule:
    - cron: '0 8 * * *'
  workflow_dispatch:
    inputs:
      mode:
        description: 'Run mode'
        required: true
        default: 'prod'
        type: choice
        options: [prod, dev]
      skip_crawl:
        description: 'Skip crawling'
        required: false
        type: boolean
        default: false
      force_recrawl:
        description: 'Force recrawl stale properties'
        required: false
        type: boolean
        default: false
      url:
        description: 'Optional single location URL'
        required: false
        type: string
        default: ''

jobs:
  scrape_and_visualize:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetches full history so rebase/push works reliably

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Dependencies
        run: |
          pip install -r requirements.txt
          pip install scikit-learn 
          playwright install chromium --with-deps

      - name: Run Unified Scraper
        if: github.event.inputs.skip_crawl != 'true'
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          P4N_USERNAME: ${{ secrets.P4N_USERNAME }}
          P4N_PASSWORD: ${{ secrets.P4N_PASSWORD }}
        run: |
          ARGS=()
          if [[ "${{ github.event.inputs.mode }}" == "dev" ]]; then ARGS+=("--dev"); fi
          if [[ "${{ github.event.inputs.force_recrawl }}" == "true" ]]; then ARGS+=("--force"); fi
          if [[ -n "${{ github.event.inputs.url }}" ]]; then ARGS+=("--url" "${{ github.event.inputs.url }}"); fi
          python backbone_crawler.py "${ARGS[@]}"

      - name: Set Intelligence Environment
        run: |
          if [ -f "backbone_locations_dev.csv" ]; then
            echo "INPUT_CSV=backbone_locations_dev.csv" >> $GITHUB_ENV
            echo "CSV_FILE=backbone_locations_dev.csv" >> $GITHUB_ENV
          else
            echo "INPUT_CSV=backbone_locations.csv" >> $GITHUB_ENV
            echo "CSV_FILE=backbone_locations.csv" >> $GITHUB_ENV
          fi

      - name: Run Demand Analyzer
        run: python demand_analyzer.py

      - name: Generate Visualization
        run: python visualize_land.py

      - name: Create CNAME
        run: echo "auroreandpedro.com" > CNAME

      - name: Upload Pipeline Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-debug-package
          path: |
            pipeline_execution.log
            strategic_analysis.json
            *.csv
            index.html
            CNAME

      - name: Commit and Push (Prod Only)
        # Runs on schedule OR if mode is NOT dev
        if: github.event_name == 'schedule' || github.event.inputs.mode == 'prod'
        run: |
          git config --global user.name "P4N Bot"
          git config --global user.email "bot@github.com"
          
          # Defensive add: only add files that actually exist
          for file in backbone_locations.csv index.html queue_state.json strategic_analysis.json; do
            if [ -f "$file" ]; then git add "$file"; fi
          done
          
          if ! git diff --staged --quiet; then
            git commit -m "Auto-update strategic intelligence [skip ci]"
            git pull --rebase origin main
            git push origin main
          else
            echo "No changes detected. Skipping push."
          fi
